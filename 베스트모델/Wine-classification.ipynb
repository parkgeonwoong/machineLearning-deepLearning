{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Wine-classification.ipynb","provenance":[],"authorship_tag":"ABX9TyMNtK/jftRsdyz0pr8muBVO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"CH3KQvnnxeXn","executionInfo":{"status":"ok","timestamp":1638426777620,"user_tz":-540,"elapsed":336,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","import pandas as pd\n","from keras.callbacks import ModelCheckpoint"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFk3xCNBxrx8","executionInfo":{"status":"ok","timestamp":1638426779446,"user_tz":-540,"elapsed":332,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}},"outputId":"3f2bd62e-10a1-445e-d968-bfc87af7c010"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"DN9gkue6xuXj","executionInfo":{"status":"ok","timestamp":1638426781179,"user_tz":-540,"elapsed":315,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}}},"source":["filename = '/content/drive/My Drive/Colab Notebooks/dataset/wine.csv'"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"s4piHPJlx64u","executionInfo":{"status":"ok","timestamp":1638426782935,"user_tz":-540,"elapsed":5,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}}},"source":["df_pre = pd.read_csv(filename)\n","df = df_pre.sample(frac=1) # 전체 데이터의 몇 % 사용할지를 "],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqSKAMLcyDQE","executionInfo":{"status":"ok","timestamp":1638426783722,"user_tz":-540,"elapsed":5,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}},"outputId":"d857ffba-20bb-4430-e89e-8e60be227c84"},"source":["print(df.head(5))"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["      7.4    0.7     0   1.9  0.076    11  ...   0.9978  3.51  0.56   9.4  5  1\n","4518  7.4  0.280  0.40  11.9  0.032  13.0  ...  0.99629  3.01  0.46  10.8  4  0\n","1167  6.5  0.340  0.27   2.8  0.067   8.0  ...  0.99384  3.21  0.56  12.0  6  1\n","846   7.4  0.680  0.16   1.8  0.078  12.0  ...  0.99770  3.50  0.70   9.9  6  1\n","3133  8.5  0.150  0.49   1.5  0.031  17.0  ...  0.99320  3.03  0.40  10.3  6  0\n","1332  9.1  0.775  0.22   2.2  0.079  12.0  ...  0.99760  3.18  0.51   9.6  5  1\n","\n","[5 rows x 13 columns]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fncpM5DyWBc","executionInfo":{"status":"ok","timestamp":1638426784014,"user_tz":-540,"elapsed":3,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}},"outputId":"647a4197-84a5-49b6-b117-4255e28b3564"},"source":["print(df.info())"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 6496 entries, 4518 to 1238\n","Data columns (total 13 columns):\n"," #   Column  Non-Null Count  Dtype  \n","---  ------  --------------  -----  \n"," 0   7.4     6496 non-null   float64\n"," 1   0.7     6496 non-null   float64\n"," 2   0       6496 non-null   float64\n"," 3   1.9     6496 non-null   float64\n"," 4   0.076   6496 non-null   float64\n"," 5   11      6496 non-null   float64\n"," 6   34      6496 non-null   float64\n"," 7   0.9978  6496 non-null   float64\n"," 8   3.51    6496 non-null   float64\n"," 9   0.56    6496 non-null   float64\n"," 10  9.4     6496 non-null   float64\n"," 11  5       6496 non-null   int64  \n"," 12  1       6496 non-null   int64  \n","dtypes: float64(11), int64(2)\n","memory usage: 710.5 KB\n","None\n"]}]},{"cell_type":"code","metadata":{"id":"NM0CDNyOyjsd","executionInfo":{"status":"ok","timestamp":1638426784302,"user_tz":-540,"elapsed":1,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}}},"source":["dataset = df.values\n","X = dataset[:, 0:12]\n","Y = dataset[:, 12]"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p3bp_QIay9MT","executionInfo":{"status":"ok","timestamp":1638426785030,"user_tz":-540,"elapsed":371,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}},"outputId":"17cc19ed-9c95-48d6-82f8-eaf1d648c4be"},"source":["model = Sequential()\n","model.add(Dense(30, input_dim=12, activation='relu'))\n","model.add(Dense(12, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_4 (Dense)             (None, 30)                390       \n","                                                                 \n"," dense_5 (Dense)             (None, 12)                372       \n","                                                                 \n"," dense_6 (Dense)             (None, 8)                 104       \n","                                                                 \n"," dense_7 (Dense)             (None, 1)                 9         \n","                                                                 \n","=================================================================\n","Total params: 875\n","Trainable params: 875\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"7gus6c26zXtd","executionInfo":{"status":"ok","timestamp":1638426786107,"user_tz":-540,"elapsed":4,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}}},"source":["model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"b3_RE-QO0S1Q","executionInfo":{"status":"ok","timestamp":1638427170690,"user_tz":-540,"elapsed":340,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}}},"source":["# 모델 업데이트하기 - 에포크마다 모델의 정확도 저장\n","import os\n","MODEL_DIR = '/content/drive/My Drive/Colab Notebooks/model/'\n","if not os.path.exists(MODEL_DIR):\n","   os.mkdir(MODEL_DIR)"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"oGAKhGey0so3","executionInfo":{"status":"ok","timestamp":1638427172241,"user_tz":-540,"elapsed":29,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}}},"source":["modelpath = '/content/drive/My Drive/Colab Notebooks/model/{epoch:02d}-{val_loss:4f}.hdf5'"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"o0LNqgCv1wxt","executionInfo":{"status":"ok","timestamp":1638427172242,"user_tz":-540,"elapsed":6,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}}},"source":["checkpointer = ModelCheckpoint(filepath=modelpath, monitor = 'val_loss', verbose=1, save_best_only=True)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yzAf4xBHzkh4","executionInfo":{"status":"ok","timestamp":1638427192599,"user_tz":-540,"elapsed":19139,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}},"outputId":"1f212617-a4b5-4e03-8974-1b2386226ab7"},"source":["history = model.fit(X, Y, validation_split=0.2,epochs=200, batch_size=200, callbacks=[checkpointer])"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0704 - accuracy: 0.9900\n","Epoch 00001: val_loss improved from inf to 0.04457, saving model to /content/drive/My Drive/Colab Notebooks/model/01-0.044570.hdf5\n","26/26 [==============================] - 0s 6ms/step - loss: 0.0438 - accuracy: 0.9879 - val_loss: 0.0446 - val_accuracy: 0.9838\n","Epoch 2/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0644 - accuracy: 0.9900\n","Epoch 00002: val_loss improved from 0.04457 to 0.04247, saving model to /content/drive/My Drive/Colab Notebooks/model/02-0.042473.hdf5\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0439 - accuracy: 0.9881 - val_loss: 0.0425 - val_accuracy: 0.9831\n","Epoch 3/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0208 - accuracy: 0.9850\n","Epoch 00003: val_loss did not improve from 0.04247\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0437 - accuracy: 0.9877 - val_loss: 0.0471 - val_accuracy: 0.9854\n","Epoch 4/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9861\n","Epoch 00004: val_loss improved from 0.04247 to 0.03740, saving model to /content/drive/My Drive/Colab Notebooks/model/04-0.037395.hdf5\n","26/26 [==============================] - 0s 5ms/step - loss: 0.0498 - accuracy: 0.9861 - val_loss: 0.0374 - val_accuracy: 0.9854\n","Epoch 5/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.1046 - accuracy: 0.9750\n","Epoch 00005: val_loss did not improve from 0.03740\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9886 - val_loss: 0.0375 - val_accuracy: 0.9854\n","Epoch 6/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0552 - accuracy: 0.9800\n","Epoch 00006: val_loss did not improve from 0.03740\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9865 - val_loss: 0.0403 - val_accuracy: 0.9838\n","Epoch 7/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0210 - accuracy: 0.9900\n","Epoch 00007: val_loss did not improve from 0.03740\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9886 - val_loss: 0.0377 - val_accuracy: 0.9854\n","Epoch 8/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0407 - accuracy: 0.9850\n","Epoch 00008: val_loss improved from 0.03740 to 0.03730, saving model to /content/drive/My Drive/Colab Notebooks/model/08-0.037300.hdf5\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0433 - accuracy: 0.9881 - val_loss: 0.0373 - val_accuracy: 0.9900\n","Epoch 9/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0554 - accuracy: 0.9900\n","Epoch 00009: val_loss did not improve from 0.03730\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.9885 - val_loss: 0.0384 - val_accuracy: 0.9854\n","Epoch 10/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9750\n","Epoch 00010: val_loss improved from 0.03730 to 0.03549, saving model to /content/drive/My Drive/Colab Notebooks/model/10-0.035491.hdf5\n","26/26 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.9890 - val_loss: 0.0355 - val_accuracy: 0.9869\n","Epoch 11/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9885\n","Epoch 00011: val_loss improved from 0.03549 to 0.03515, saving model to /content/drive/My Drive/Colab Notebooks/model/11-0.035151.hdf5\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0424 - accuracy: 0.9885 - val_loss: 0.0352 - val_accuracy: 0.9862\n","Epoch 12/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0490 - accuracy: 0.9900\n","Epoch 00012: val_loss improved from 0.03515 to 0.03513, saving model to /content/drive/My Drive/Colab Notebooks/model/12-0.035134.hdf5\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0416 - accuracy: 0.9894 - val_loss: 0.0351 - val_accuracy: 0.9900\n","Epoch 13/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0291 - accuracy: 0.9900\n","Epoch 00013: val_loss did not improve from 0.03513\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9848 - val_loss: 0.0466 - val_accuracy: 0.9877\n","Epoch 14/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0279 - accuracy: 1.0000\n","Epoch 00014: val_loss did not improve from 0.03513\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9881 - val_loss: 0.0361 - val_accuracy: 0.9854\n","Epoch 15/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0235 - accuracy: 0.9900\n","Epoch 00015: val_loss improved from 0.03513 to 0.03503, saving model to /content/drive/My Drive/Colab Notebooks/model/15-0.035026.hdf5\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0428 - accuracy: 0.9888 - val_loss: 0.0350 - val_accuracy: 0.9908\n","Epoch 16/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9863\n","Epoch 00016: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.9863 - val_loss: 0.0453 - val_accuracy: 0.9854\n","Epoch 17/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.1002 - accuracy: 0.9750\n","Epoch 00017: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.9863 - val_loss: 0.0369 - val_accuracy: 0.9900\n","Epoch 18/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0253 - accuracy: 0.9900\n","Epoch 00018: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.9890 - val_loss: 0.0364 - val_accuracy: 0.9885\n","Epoch 19/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0210 - accuracy: 0.9950\n","Epoch 00019: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 0.9875 - val_loss: 0.0384 - val_accuracy: 0.9854\n","Epoch 20/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0309 - accuracy: 0.9900\n","Epoch 00020: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0420 - accuracy: 0.9892 - val_loss: 0.0393 - val_accuracy: 0.9900\n","Epoch 21/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0429 - accuracy: 0.9950\n","Epoch 00021: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.9871 - val_loss: 0.0360 - val_accuracy: 0.9854\n","Epoch 22/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n","Epoch 00022: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0480 - accuracy: 0.9875 - val_loss: 0.0364 - val_accuracy: 0.9862\n","Epoch 23/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0111 - accuracy: 0.9950\n","Epoch 00023: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0433 - accuracy: 0.9871 - val_loss: 0.0361 - val_accuracy: 0.9892\n","Epoch 24/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9881\n","Epoch 00024: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.9881 - val_loss: 0.0406 - val_accuracy: 0.9831\n","Epoch 25/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0477 - accuracy: 0.9900\n","Epoch 00025: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0434 - accuracy: 0.9883 - val_loss: 0.0459 - val_accuracy: 0.9838\n","Epoch 26/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9881\n","Epoch 00026: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0454 - accuracy: 0.9881 - val_loss: 0.0367 - val_accuracy: 0.9892\n","Epoch 27/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0211 - accuracy: 0.9900\n","Epoch 00027: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.9879 - val_loss: 0.0385 - val_accuracy: 0.9869\n","Epoch 28/200\n","24/26 [==========================>...] - ETA: 0s - loss: 0.0451 - accuracy: 0.9860\n","Epoch 00028: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0426 - accuracy: 0.9867 - val_loss: 0.0429 - val_accuracy: 0.9831\n","Epoch 29/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0591 - accuracy: 0.9800\n","Epoch 00029: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0429 - accuracy: 0.9885 - val_loss: 0.0356 - val_accuracy: 0.9869\n","Epoch 30/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0471 - accuracy: 0.9900\n","Epoch 00030: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9883 - val_loss: 0.0375 - val_accuracy: 0.9892\n","Epoch 31/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0460 - accuracy: 0.9800\n","Epoch 00031: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0422 - accuracy: 0.9881 - val_loss: 0.0386 - val_accuracy: 0.9838\n","Epoch 32/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0226 - accuracy: 0.9950\n","Epoch 00032: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0410 - accuracy: 0.9886 - val_loss: 0.0376 - val_accuracy: 0.9846\n","Epoch 33/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n","Epoch 00033: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0415 - accuracy: 0.9894 - val_loss: 0.0359 - val_accuracy: 0.9862\n","Epoch 34/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0890 - accuracy: 0.9750\n","Epoch 00034: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0408 - accuracy: 0.9892 - val_loss: 0.0375 - val_accuracy: 0.9838\n","Epoch 35/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0267 - accuracy: 0.9850\n","Epoch 00035: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9879 - val_loss: 0.0356 - val_accuracy: 0.9900\n","Epoch 36/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0460 - accuracy: 0.9900\n","Epoch 00036: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9883 - val_loss: 0.0430 - val_accuracy: 0.9854\n","Epoch 37/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0194 - accuracy: 0.9950\n","Epoch 00037: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.9863 - val_loss: 0.0359 - val_accuracy: 0.9862\n","Epoch 38/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0298 - accuracy: 0.9950\n","Epoch 00038: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0414 - accuracy: 0.9888 - val_loss: 0.0439 - val_accuracy: 0.9862\n","Epoch 39/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9900\n","Epoch 00039: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9860 - val_loss: 0.0374 - val_accuracy: 0.9862\n","Epoch 40/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0239 - accuracy: 0.9900\n","Epoch 00040: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0434 - accuracy: 0.9875 - val_loss: 0.0359 - val_accuracy: 0.9869\n","Epoch 41/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0581 - accuracy: 0.9900\n","Epoch 00041: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0418 - accuracy: 0.9894 - val_loss: 0.0368 - val_accuracy: 0.9854\n","Epoch 42/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0230 - accuracy: 0.9950\n","Epoch 00042: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0459 - accuracy: 0.9873 - val_loss: 0.0509 - val_accuracy: 0.9838\n","Epoch 43/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0552 - accuracy: 0.9900\n","Epoch 00043: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0429 - accuracy: 0.9888 - val_loss: 0.0407 - val_accuracy: 0.9846\n","Epoch 44/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0275 - accuracy: 0.9850\n","Epoch 00044: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0412 - accuracy: 0.9894 - val_loss: 0.0354 - val_accuracy: 0.9869\n","Epoch 45/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0260 - accuracy: 0.9900\n","Epoch 00045: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0431 - accuracy: 0.9875 - val_loss: 0.0360 - val_accuracy: 0.9900\n","Epoch 46/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n","Epoch 00046: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0409 - accuracy: 0.9900 - val_loss: 0.0364 - val_accuracy: 0.9854\n","Epoch 47/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0162 - accuracy: 0.9950\n","Epoch 00047: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9860 - val_loss: 0.0582 - val_accuracy: 0.9800\n","Epoch 48/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0116 - accuracy: 1.0000\n","Epoch 00048: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 0.9881 - val_loss: 0.0387 - val_accuracy: 0.9854\n","Epoch 49/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0227 - accuracy: 0.9950\n","Epoch 00049: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9886 - val_loss: 0.0377 - val_accuracy: 0.9892\n","Epoch 50/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0438 - accuracy: 0.9900\n","Epoch 00050: val_loss did not improve from 0.03503\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 0.9888 - val_loss: 0.0379 - val_accuracy: 0.9854\n","Epoch 51/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0194 - accuracy: 0.9850\n","Epoch 00051: val_loss improved from 0.03503 to 0.03385, saving model to /content/drive/My Drive/Colab Notebooks/model/51-0.033847.hdf5\n","26/26 [==============================] - 0s 5ms/step - loss: 0.0408 - accuracy: 0.9888 - val_loss: 0.0338 - val_accuracy: 0.9869\n","Epoch 52/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0344 - accuracy: 0.9900\n","Epoch 00052: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.9881 - val_loss: 0.0359 - val_accuracy: 0.9915\n","Epoch 53/200\n","21/26 [=======================>......] - ETA: 0s - loss: 0.0395 - accuracy: 0.9874\n","Epoch 00053: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.9867 - val_loss: 0.0369 - val_accuracy: 0.9892\n","Epoch 54/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0439 - accuracy: 0.9850\n","Epoch 00054: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0434 - accuracy: 0.9877 - val_loss: 0.0386 - val_accuracy: 0.9846\n","Epoch 55/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0369 - accuracy: 0.9950\n","Epoch 00055: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9875 - val_loss: 0.0353 - val_accuracy: 0.9854\n","Epoch 56/200\n","23/26 [=========================>....] - ETA: 0s - loss: 0.0453 - accuracy: 0.9880\n","Epoch 00056: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0465 - accuracy: 0.9879 - val_loss: 0.0683 - val_accuracy: 0.9800\n","Epoch 57/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.1255 - accuracy: 0.9650\n","Epoch 00057: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.9879 - val_loss: 0.0362 - val_accuracy: 0.9862\n","Epoch 58/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0141 - accuracy: 0.9950\n","Epoch 00058: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0406 - accuracy: 0.9896 - val_loss: 0.0386 - val_accuracy: 0.9838\n","Epoch 59/200\n","24/26 [==========================>...] - ETA: 0s - loss: 0.0403 - accuracy: 0.9894\n","Epoch 00059: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0407 - accuracy: 0.9890 - val_loss: 0.0363 - val_accuracy: 0.9862\n","Epoch 60/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9863\n","Epoch 00060: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0435 - accuracy: 0.9863 - val_loss: 0.0354 - val_accuracy: 0.9900\n","Epoch 61/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0991 - accuracy: 0.9800\n","Epoch 00061: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0408 - accuracy: 0.9894 - val_loss: 0.0353 - val_accuracy: 0.9900\n","Epoch 62/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0412 - accuracy: 0.9950\n","Epoch 00062: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0416 - accuracy: 0.9890 - val_loss: 0.0361 - val_accuracy: 0.9877\n","Epoch 63/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0125 - accuracy: 1.0000\n","Epoch 00063: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0407 - accuracy: 0.9890 - val_loss: 0.0350 - val_accuracy: 0.9892\n","Epoch 64/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0795 - accuracy: 0.9800\n","Epoch 00064: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9856 - val_loss: 0.0488 - val_accuracy: 0.9862\n","Epoch 65/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0761 - accuracy: 0.9650\n","Epoch 00065: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 0.9865 - val_loss: 0.0352 - val_accuracy: 0.9869\n","Epoch 66/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0204 - accuracy: 0.9900\n","Epoch 00066: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0402 - accuracy: 0.9888 - val_loss: 0.0376 - val_accuracy: 0.9838\n","Epoch 67/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9871\n","Epoch 00067: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0430 - accuracy: 0.9871 - val_loss: 0.0356 - val_accuracy: 0.9892\n","Epoch 68/200\n","23/26 [=========================>....] - ETA: 0s - loss: 0.0399 - accuracy: 0.9889\n","Epoch 00068: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0403 - accuracy: 0.9890 - val_loss: 0.0351 - val_accuracy: 0.9862\n","Epoch 69/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9950\n","Epoch 00069: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0399 - accuracy: 0.9906 - val_loss: 0.0345 - val_accuracy: 0.9908\n","Epoch 70/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0187 - accuracy: 0.9950\n","Epoch 00070: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 0.9892 - val_loss: 0.0351 - val_accuracy: 0.9908\n","Epoch 71/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9883\n","Epoch 00071: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0434 - accuracy: 0.9883 - val_loss: 0.0382 - val_accuracy: 0.9846\n","Epoch 72/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0274 - accuracy: 0.9900\n","Epoch 00072: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0426 - accuracy: 0.9883 - val_loss: 0.0410 - val_accuracy: 0.9831\n","Epoch 73/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0160 - accuracy: 0.9950\n","Epoch 00073: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0441 - accuracy: 0.9886 - val_loss: 0.0354 - val_accuracy: 0.9862\n","Epoch 74/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9858\n","Epoch 00074: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9858 - val_loss: 0.0361 - val_accuracy: 0.9869\n","Epoch 75/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0466 - accuracy: 0.9800\n","Epoch 00075: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0411 - accuracy: 0.9888 - val_loss: 0.0363 - val_accuracy: 0.9862\n","Epoch 76/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0162 - accuracy: 0.9950\n","Epoch 00076: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0396 - accuracy: 0.9894 - val_loss: 0.0361 - val_accuracy: 0.9892\n","Epoch 77/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0239 - accuracy: 0.9950\n","Epoch 00077: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0409 - accuracy: 0.9886 - val_loss: 0.0340 - val_accuracy: 0.9892\n","Epoch 78/200\n","25/26 [===========================>..] - ETA: 0s - loss: 0.0423 - accuracy: 0.9894\n","Epoch 00078: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0415 - accuracy: 0.9892 - val_loss: 0.0362 - val_accuracy: 0.9854\n","Epoch 79/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0524 - accuracy: 0.9850\n","Epoch 00079: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.9888 - val_loss: 0.0355 - val_accuracy: 0.9862\n","Epoch 80/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0415 - accuracy: 0.9950\n","Epoch 00080: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0399 - accuracy: 0.9892 - val_loss: 0.0440 - val_accuracy: 0.9838\n","Epoch 81/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0095 - accuracy: 0.9950\n","Epoch 00081: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 0.9881 - val_loss: 0.0361 - val_accuracy: 0.9846\n","Epoch 82/200\n","25/26 [===========================>..] - ETA: 0s - loss: 0.0403 - accuracy: 0.9896\n","Epoch 00082: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 0.9892 - val_loss: 0.0347 - val_accuracy: 0.9869\n","Epoch 83/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0537 - accuracy: 0.9850\n","Epoch 00083: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9877 - val_loss: 0.0360 - val_accuracy: 0.9854\n","Epoch 84/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n","Epoch 00084: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0395 - accuracy: 0.9892 - val_loss: 0.0344 - val_accuracy: 0.9877\n","Epoch 85/200\n","22/26 [========================>.....] - ETA: 0s - loss: 0.0397 - accuracy: 0.9895\n","Epoch 00085: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0398 - accuracy: 0.9900 - val_loss: 0.0344 - val_accuracy: 0.9900\n","Epoch 86/200\n","20/26 [======================>.......] - ETA: 0s - loss: 0.0396 - accuracy: 0.9900\n","Epoch 00086: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 5ms/step - loss: 0.0391 - accuracy: 0.9902 - val_loss: 0.0348 - val_accuracy: 0.9869\n","Epoch 87/200\n","11/26 [===========>..................] - ETA: 0s - loss: 0.0381 - accuracy: 0.9895\n","Epoch 00087: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 6ms/step - loss: 0.0390 - accuracy: 0.9894 - val_loss: 0.0344 - val_accuracy: 0.9885\n","Epoch 88/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0746 - accuracy: 0.9700\n","Epoch 00088: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0401 - accuracy: 0.9890 - val_loss: 0.0364 - val_accuracy: 0.9854\n","Epoch 89/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0493 - accuracy: 0.9800\n","Epoch 00089: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9881 - val_loss: 0.0382 - val_accuracy: 0.9838\n","Epoch 90/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n","Epoch 00090: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 0.9896 - val_loss: 0.0343 - val_accuracy: 0.9885\n","Epoch 91/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0178 - accuracy: 0.9950\n","Epoch 00091: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0415 - accuracy: 0.9896 - val_loss: 0.0355 - val_accuracy: 0.9862\n","Epoch 92/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9950\n","Epoch 00092: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9888 - val_loss: 0.0348 - val_accuracy: 0.9869\n","Epoch 93/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0555 - accuracy: 0.9850\n","Epoch 00093: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0392 - accuracy: 0.9894 - val_loss: 0.0348 - val_accuracy: 0.9854\n","Epoch 94/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0840 - accuracy: 0.9800\n","Epoch 00094: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0415 - accuracy: 0.9885 - val_loss: 0.0384 - val_accuracy: 0.9885\n","Epoch 95/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0520 - accuracy: 0.9900\n","Epoch 00095: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9844 - val_loss: 0.0925 - val_accuracy: 0.9731\n","Epoch 96/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.1577 - accuracy: 0.9600\n","Epoch 00096: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9819 - val_loss: 0.0608 - val_accuracy: 0.9831\n","Epoch 97/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0252 - accuracy: 0.9950\n","Epoch 00097: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0500 - accuracy: 0.9863 - val_loss: 0.0390 - val_accuracy: 0.9869\n","Epoch 98/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0449 - accuracy: 0.9950\n","Epoch 00098: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0403 - accuracy: 0.9898 - val_loss: 0.0340 - val_accuracy: 0.9862\n","Epoch 99/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0996 - accuracy: 0.9850\n","Epoch 00099: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0414 - accuracy: 0.9881 - val_loss: 0.0434 - val_accuracy: 0.9831\n","Epoch 100/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0249 - accuracy: 0.9900\n","Epoch 00100: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0420 - accuracy: 0.9879 - val_loss: 0.0353 - val_accuracy: 0.9869\n","Epoch 101/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0790 - accuracy: 0.9800\n","Epoch 00101: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9892 - val_loss: 0.0346 - val_accuracy: 0.9869\n","Epoch 102/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000\n","Epoch 00102: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0384 - accuracy: 0.9900 - val_loss: 0.0356 - val_accuracy: 0.9854\n","Epoch 103/200\n","24/26 [==========================>...] - ETA: 0s - loss: 0.0353 - accuracy: 0.9915\n","Epoch 00103: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0383 - accuracy: 0.9904 - val_loss: 0.0405 - val_accuracy: 0.9838\n","Epoch 104/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0525 - accuracy: 0.9850\n","Epoch 00104: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0403 - accuracy: 0.9890 - val_loss: 0.0415 - val_accuracy: 0.9838\n","Epoch 105/200\n","20/26 [======================>.......] - ETA: 0s - loss: 0.0397 - accuracy: 0.9898\n","Epoch 00105: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.9890 - val_loss: 0.0349 - val_accuracy: 0.9854\n","Epoch 106/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0462 - accuracy: 0.9800\n","Epoch 00106: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0432 - accuracy: 0.9877 - val_loss: 0.0374 - val_accuracy: 0.9854\n","Epoch 107/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0118 - accuracy: 1.0000\n","Epoch 00107: val_loss did not improve from 0.03385\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9892 - val_loss: 0.0436 - val_accuracy: 0.9838\n","Epoch 108/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9888\n","Epoch 00108: val_loss improved from 0.03385 to 0.03283, saving model to /content/drive/My Drive/Colab Notebooks/model/108-0.032827.hdf5\n","26/26 [==============================] - 0s 5ms/step - loss: 0.0409 - accuracy: 0.9888 - val_loss: 0.0328 - val_accuracy: 0.9892\n","Epoch 109/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9890\n","Epoch 00109: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0403 - accuracy: 0.9890 - val_loss: 0.0369 - val_accuracy: 0.9838\n","Epoch 110/200\n","23/26 [=========================>....] - ETA: 0s - loss: 0.0396 - accuracy: 0.9898\n","Epoch 00110: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9900 - val_loss: 0.0330 - val_accuracy: 0.9900\n","Epoch 111/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0403 - accuracy: 0.9950\n","Epoch 00111: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0432 - accuracy: 0.9885 - val_loss: 0.0512 - val_accuracy: 0.9823\n","Epoch 112/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0892 - accuracy: 0.9800\n","Epoch 00112: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0410 - accuracy: 0.9894 - val_loss: 0.0349 - val_accuracy: 0.9854\n","Epoch 113/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9896\n","Epoch 00113: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0405 - accuracy: 0.9896 - val_loss: 0.0385 - val_accuracy: 0.9838\n","Epoch 114/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0742 - accuracy: 0.9800\n","Epoch 00114: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0411 - accuracy: 0.9886 - val_loss: 0.0455 - val_accuracy: 0.9838\n","Epoch 115/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0272 - accuracy: 0.9900\n","Epoch 00115: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0432 - accuracy: 0.9877 - val_loss: 0.0382 - val_accuracy: 0.9854\n","Epoch 116/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0427 - accuracy: 0.9900\n","Epoch 00116: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0384 - accuracy: 0.9900 - val_loss: 0.0345 - val_accuracy: 0.9892\n","Epoch 117/200\n","22/26 [========================>.....] - ETA: 0s - loss: 0.0400 - accuracy: 0.9898\n","Epoch 00117: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.9892 - val_loss: 0.0372 - val_accuracy: 0.9862\n","Epoch 118/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0332 - accuracy: 0.9900\n","Epoch 00118: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0418 - accuracy: 0.9881 - val_loss: 0.0332 - val_accuracy: 0.9892\n","Epoch 119/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0203 - accuracy: 0.9950\n","Epoch 00119: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9906 - val_loss: 0.0341 - val_accuracy: 0.9862\n","Epoch 120/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0167 - accuracy: 0.9950\n","Epoch 00120: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0430 - accuracy: 0.9877 - val_loss: 0.0443 - val_accuracy: 0.9838\n","Epoch 121/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0317 - accuracy: 0.9850\n","Epoch 00121: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0401 - accuracy: 0.9888 - val_loss: 0.0340 - val_accuracy: 0.9900\n","Epoch 122/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0256 - accuracy: 0.9950\n","Epoch 00122: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0414 - accuracy: 0.9896 - val_loss: 0.0406 - val_accuracy: 0.9838\n","Epoch 123/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0629 - accuracy: 0.9850\n","Epoch 00123: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0400 - accuracy: 0.9888 - val_loss: 0.0340 - val_accuracy: 0.9900\n","Epoch 124/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0887 - accuracy: 0.9700\n","Epoch 00124: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0395 - accuracy: 0.9904 - val_loss: 0.0352 - val_accuracy: 0.9900\n","Epoch 125/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0584 - accuracy: 0.9850\n","Epoch 00125: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0422 - accuracy: 0.9898 - val_loss: 0.0336 - val_accuracy: 0.9892\n","Epoch 126/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0173 - accuracy: 0.9950\n","Epoch 00126: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.9910 - val_loss: 0.0341 - val_accuracy: 0.9862\n","Epoch 127/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0924 - accuracy: 0.9750\n","Epoch 00127: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 0.9896 - val_loss: 0.0340 - val_accuracy: 0.9892\n","Epoch 128/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n","Epoch 00128: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9902 - val_loss: 0.0386 - val_accuracy: 0.9846\n","Epoch 129/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0492 - accuracy: 0.9800\n","Epoch 00129: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.9904 - val_loss: 0.0347 - val_accuracy: 0.9862\n","Epoch 130/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0111 - accuracy: 1.0000\n","Epoch 00130: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0389 - accuracy: 0.9890 - val_loss: 0.0339 - val_accuracy: 0.9885\n","Epoch 131/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0186 - accuracy: 0.9900\n","Epoch 00131: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 0.9902 - val_loss: 0.0364 - val_accuracy: 0.9854\n","Epoch 132/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0431 - accuracy: 0.9900\n","Epoch 00132: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0406 - accuracy: 0.9885 - val_loss: 0.0341 - val_accuracy: 0.9892\n","Epoch 133/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9879\n","Epoch 00133: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0427 - accuracy: 0.9879 - val_loss: 0.0345 - val_accuracy: 0.9892\n","Epoch 134/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.1180 - accuracy: 0.9700\n","Epoch 00134: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0372 - accuracy: 0.9904 - val_loss: 0.0329 - val_accuracy: 0.9862\n","Epoch 135/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0162 - accuracy: 1.0000\n","Epoch 00135: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0401 - accuracy: 0.9888 - val_loss: 0.0386 - val_accuracy: 0.9838\n","Epoch 136/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0376 - accuracy: 0.9850\n","Epoch 00136: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0400 - accuracy: 0.9890 - val_loss: 0.0366 - val_accuracy: 0.9885\n","Epoch 137/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9850\n","Epoch 00137: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0408 - accuracy: 0.9888 - val_loss: 0.0339 - val_accuracy: 0.9900\n","Epoch 138/200\n","22/26 [========================>.....] - ETA: 0s - loss: 0.0399 - accuracy: 0.9886\n","Epoch 00138: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0381 - accuracy: 0.9890 - val_loss: 0.0375 - val_accuracy: 0.9846\n","Epoch 139/200\n","19/26 [====================>.........] - ETA: 0s - loss: 0.0407 - accuracy: 0.9889\n","Epoch 00139: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0429 - accuracy: 0.9886 - val_loss: 0.0462 - val_accuracy: 0.9838\n","Epoch 140/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0391 - accuracy: 0.9900\n","Epoch 00140: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0398 - accuracy: 0.9898 - val_loss: 0.0457 - val_accuracy: 0.9831\n","Epoch 141/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0468 - accuracy: 0.9900\n","Epoch 00141: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0399 - accuracy: 0.9902 - val_loss: 0.0368 - val_accuracy: 0.9854\n","Epoch 142/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0509 - accuracy: 0.9900\n","Epoch 00142: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0371 - accuracy: 0.9898 - val_loss: 0.0361 - val_accuracy: 0.9885\n","Epoch 143/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0219 - accuracy: 0.9900\n","Epoch 00143: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0378 - accuracy: 0.9900 - val_loss: 0.0330 - val_accuracy: 0.9862\n","Epoch 144/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0747 - accuracy: 0.9850\n","Epoch 00144: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0390 - accuracy: 0.9896 - val_loss: 0.0428 - val_accuracy: 0.9908\n","Epoch 145/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0400 - accuracy: 0.9950\n","Epoch 00145: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9852 - val_loss: 0.0364 - val_accuracy: 0.9892\n","Epoch 146/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0637 - accuracy: 0.9850\n","Epoch 00146: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0412 - accuracy: 0.9888 - val_loss: 0.0373 - val_accuracy: 0.9846\n","Epoch 147/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0847 - accuracy: 0.9850\n","Epoch 00147: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0396 - accuracy: 0.9877 - val_loss: 0.0374 - val_accuracy: 0.9846\n","Epoch 148/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0129 - accuracy: 0.9950\n","Epoch 00148: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0414 - accuracy: 0.9894 - val_loss: 0.0463 - val_accuracy: 0.9838\n","Epoch 149/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0584 - accuracy: 0.9800\n","Epoch 00149: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9869 - val_loss: 0.0341 - val_accuracy: 0.9854\n","Epoch 150/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0260 - accuracy: 0.9900\n","Epoch 00150: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0373 - accuracy: 0.9906 - val_loss: 0.0342 - val_accuracy: 0.9862\n","Epoch 151/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.1106 - accuracy: 0.9750\n","Epoch 00151: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0376 - accuracy: 0.9900 - val_loss: 0.0335 - val_accuracy: 0.9900\n","Epoch 152/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0411 - accuracy: 0.9900\n","Epoch 00152: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0373 - accuracy: 0.9908 - val_loss: 0.0388 - val_accuracy: 0.9831\n","Epoch 153/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0486 - accuracy: 0.9850\n","Epoch 00153: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.9894 - val_loss: 0.0384 - val_accuracy: 0.9838\n","Epoch 154/200\n","24/26 [==========================>...] - ETA: 0s - loss: 0.0371 - accuracy: 0.9904\n","Epoch 00154: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0383 - accuracy: 0.9900 - val_loss: 0.0338 - val_accuracy: 0.9869\n","Epoch 155/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0182 - accuracy: 0.9950\n","Epoch 00155: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 0.9886 - val_loss: 0.0384 - val_accuracy: 0.9900\n","Epoch 156/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9860\n","Epoch 00156: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0503 - accuracy: 0.9860 - val_loss: 0.0479 - val_accuracy: 0.9846\n","Epoch 157/200\n","25/26 [===========================>..] - ETA: 0s - loss: 0.0411 - accuracy: 0.9888\n","Epoch 00157: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.9885 - val_loss: 0.0351 - val_accuracy: 0.9892\n","Epoch 158/200\n","23/26 [=========================>....] - ETA: 0s - loss: 0.0413 - accuracy: 0.9893\n","Epoch 00158: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9888 - val_loss: 0.0340 - val_accuracy: 0.9862\n","Epoch 159/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0490 - accuracy: 0.9950\n","Epoch 00159: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0406 - accuracy: 0.9888 - val_loss: 0.0454 - val_accuracy: 0.9838\n","Epoch 160/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0432 - accuracy: 0.9800\n","Epoch 00160: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.9879 - val_loss: 0.0347 - val_accuracy: 0.9862\n","Epoch 161/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0286 - accuracy: 0.9850\n","Epoch 00161: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0369 - accuracy: 0.9902 - val_loss: 0.0413 - val_accuracy: 0.9838\n","Epoch 162/200\n","21/26 [=======================>......] - ETA: 0s - loss: 0.0389 - accuracy: 0.9886\n","Epoch 00162: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0391 - accuracy: 0.9894 - val_loss: 0.0333 - val_accuracy: 0.9869\n","Epoch 163/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0237 - accuracy: 0.9850\n","Epoch 00163: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0376 - accuracy: 0.9902 - val_loss: 0.0343 - val_accuracy: 0.9869\n","Epoch 164/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0177 - accuracy: 0.9950\n","Epoch 00164: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0364 - accuracy: 0.9904 - val_loss: 0.0352 - val_accuracy: 0.9908\n","Epoch 165/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0921 - accuracy: 0.9800\n","Epoch 00165: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0365 - accuracy: 0.9904 - val_loss: 0.0333 - val_accuracy: 0.9862\n","Epoch 166/200\n","22/26 [========================>.....] - ETA: 0s - loss: 0.0383 - accuracy: 0.9909\n","Epoch 00166: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.9908 - val_loss: 0.0366 - val_accuracy: 0.9854\n","Epoch 167/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0420 - accuracy: 0.9900\n","Epoch 00167: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0371 - accuracy: 0.9898 - val_loss: 0.0396 - val_accuracy: 0.9838\n","Epoch 168/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9896\n","Epoch 00168: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.9896 - val_loss: 0.0342 - val_accuracy: 0.9854\n","Epoch 169/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0198 - accuracy: 0.9950\n","Epoch 00169: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 0.9885 - val_loss: 0.0349 - val_accuracy: 0.9892\n","Epoch 170/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0259 - accuracy: 0.9950\n","Epoch 00170: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9898 - val_loss: 0.0330 - val_accuracy: 0.9900\n","Epoch 171/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9894\n","Epoch 00171: val_loss did not improve from 0.03283\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0389 - accuracy: 0.9894 - val_loss: 0.0362 - val_accuracy: 0.9854\n","Epoch 172/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0269 - accuracy: 0.9900\n","Epoch 00172: val_loss improved from 0.03283 to 0.03245, saving model to /content/drive/My Drive/Colab Notebooks/model/172-0.032447.hdf5\n","26/26 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 0.9911 - val_loss: 0.0324 - val_accuracy: 0.9877\n","Epoch 173/200\n","25/26 [===========================>..] - ETA: 0s - loss: 0.0375 - accuracy: 0.9898\n","Epoch 00173: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 0.9896 - val_loss: 0.0371 - val_accuracy: 0.9908\n","Epoch 174/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0240 - accuracy: 1.0000\n","Epoch 00174: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0375 - accuracy: 0.9904 - val_loss: 0.0389 - val_accuracy: 0.9838\n","Epoch 175/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0378 - accuracy: 0.9800\n","Epoch 00175: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 0.9892 - val_loss: 0.0369 - val_accuracy: 0.9900\n","Epoch 176/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0649 - accuracy: 0.9850\n","Epoch 00176: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9888 - val_loss: 0.0369 - val_accuracy: 0.9915\n","Epoch 177/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0838 - accuracy: 0.9750\n","Epoch 00177: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0406 - accuracy: 0.9881 - val_loss: 0.0431 - val_accuracy: 0.9900\n","Epoch 178/200\n","23/26 [=========================>....] - ETA: 0s - loss: 0.0389 - accuracy: 0.9883\n","Epoch 00178: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9883 - val_loss: 0.0402 - val_accuracy: 0.9838\n","Epoch 179/200\n","21/26 [=======================>......] - ETA: 0s - loss: 0.0392 - accuracy: 0.9886\n","Epoch 00179: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0390 - accuracy: 0.9888 - val_loss: 0.0362 - val_accuracy: 0.9846\n","Epoch 180/200\n","23/26 [=========================>....] - ETA: 0s - loss: 0.0354 - accuracy: 0.9902\n","Epoch 00180: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.9902 - val_loss: 0.0326 - val_accuracy: 0.9885\n","Epoch 181/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0209 - accuracy: 0.9950\n","Epoch 00181: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.9900 - val_loss: 0.0344 - val_accuracy: 0.9892\n","Epoch 182/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9894\n","Epoch 00182: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.9894 - val_loss: 0.0349 - val_accuracy: 0.9877\n","Epoch 183/200\n","23/26 [=========================>....] - ETA: 0s - loss: 0.0414 - accuracy: 0.9880\n","Epoch 00183: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0326 - val_accuracy: 0.9892\n","Epoch 184/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0361 - accuracy: 0.9850\n","Epoch 00184: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9885 - val_loss: 0.0345 - val_accuracy: 0.9862\n","Epoch 185/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0380 - accuracy: 0.9950\n","Epoch 00185: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0384 - accuracy: 0.9892 - val_loss: 0.0340 - val_accuracy: 0.9869\n","Epoch 186/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0543 - accuracy: 0.9900\n","Epoch 00186: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0368 - accuracy: 0.9896 - val_loss: 0.0344 - val_accuracy: 0.9854\n","Epoch 187/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0361 - accuracy: 0.9950\n","Epoch 00187: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.9908 - val_loss: 0.0372 - val_accuracy: 0.9838\n","Epoch 188/200\n","24/26 [==========================>...] - ETA: 0s - loss: 0.0366 - accuracy: 0.9902\n","Epoch 00188: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.9902 - val_loss: 0.0329 - val_accuracy: 0.9900\n","Epoch 189/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0215 - accuracy: 0.9950\n","Epoch 00189: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 0.9913 - val_loss: 0.0334 - val_accuracy: 0.9900\n","Epoch 190/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n","Epoch 00190: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.9900 - val_loss: 0.0411 - val_accuracy: 0.9831\n","Epoch 191/200\n","23/26 [=========================>....] - ETA: 0s - loss: 0.0363 - accuracy: 0.9904\n","Epoch 00191: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0362 - accuracy: 0.9906 - val_loss: 0.0329 - val_accuracy: 0.9900\n","Epoch 192/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9888\n","Epoch 00192: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.9888 - val_loss: 0.0336 - val_accuracy: 0.9885\n","Epoch 193/200\n","21/26 [=======================>......] - ETA: 0s - loss: 0.0395 - accuracy: 0.9888\n","Epoch 00193: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9892 - val_loss: 0.0354 - val_accuracy: 0.9854\n","Epoch 194/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9906\n","Epoch 00194: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0370 - accuracy: 0.9906 - val_loss: 0.0361 - val_accuracy: 0.9846\n","Epoch 195/200\n","26/26 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9906\n","Epoch 00195: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0374 - accuracy: 0.9906 - val_loss: 0.0325 - val_accuracy: 0.9885\n","Epoch 196/200\n","21/26 [=======================>......] - ETA: 0s - loss: 0.0375 - accuracy: 0.9905\n","Epoch 00196: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.9906 - val_loss: 0.0353 - val_accuracy: 0.9854\n","Epoch 197/200\n"," 1/26 [>.............................] - ETA: 0s - loss: 0.0371 - accuracy: 0.9950\n","Epoch 00197: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 3ms/step - loss: 0.0361 - accuracy: 0.9902 - val_loss: 0.0408 - val_accuracy: 0.9831\n","Epoch 198/200\n","25/26 [===========================>..] - ETA: 0s - loss: 0.0354 - accuracy: 0.9898\n","Epoch 00198: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 0.9894 - val_loss: 0.0342 - val_accuracy: 0.9869\n","Epoch 199/200\n","25/26 [===========================>..] - ETA: 0s - loss: 0.0380 - accuracy: 0.9892\n","Epoch 00199: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0372 - accuracy: 0.9896 - val_loss: 0.0331 - val_accuracy: 0.9892\n","Epoch 200/200\n","25/26 [===========================>..] - ETA: 0s - loss: 0.0349 - accuracy: 0.9904\n","Epoch 00200: val_loss did not improve from 0.03245\n","26/26 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.9900 - val_loss: 0.0352 - val_accuracy: 0.9854\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWDAJn_uzpqv","executionInfo":{"status":"ok","timestamp":1638426185374,"user_tz":-540,"elapsed":804,"user":{"displayName":"박건웅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09250938170526249871"}},"outputId":"6afcae98-5e85-4cbe-fdd8-2d08ab814871"},"source":["# 결과 출력\n","print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["203/203 [==============================] - 0s 1ms/step - loss: 0.0541 - accuracy: 0.9831\n","\n"," Accuracy: 0.9831\n"]}]},{"cell_type":"code","metadata":{"id":"p2lNQe8Gz4jt"},"source":[""],"execution_count":null,"outputs":[]}]}